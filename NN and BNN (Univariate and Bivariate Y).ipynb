{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9577cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Linear Model MSE: 0.34422797\n",
      "Epoch 200, NLL: 11899.8330\n",
      "Epoch 400, NLL: 2307.1782\n",
      "Epoch 600, NLL: -849.7971\n",
      "Epoch 800, NLL: -1542.2004\n",
      "Epoch 1000, NLL: -1681.6061\n",
      "NN + Random Effects MSE: 0.010102031\n",
      "Comparison of Models:\n",
      " - Baseline Linear Model MSE: 0.3442\n",
      " - NN + Random Effects MSE: 0.0101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#############################################\n",
    "# Data Generation\n",
    "#############################################\n",
    "\n",
    "# Parameters\n",
    "N = 100    # number of subjects\n",
    "T = 20     # number of time points per subject\n",
    "p = 1      # dimension of random effects (e.g., random intercept only)\n",
    "sigma = 0.1   # residual std dev\n",
    "tau = 0.5     # std dev of random effects\n",
    "\n",
    "# Generate the main predictor variable\n",
    "# We'll have a single primary predictor x, and from it we'll create 4 transformations:\n",
    "# X1 = x, X2 = cos(x), X3 = sin^2(x), X4 = x^2\n",
    "x_vals = np.linspace(0, 10, T)\n",
    "X_main = np.tile(x_vals, (N, 1))  # shape (N, T)\n",
    "\n",
    "# Construct the full design matrix for fixed effects as input to the NN:\n",
    "# We'll have W as random covariates (here we can just simulate some additional random covariates)\n",
    "W = np.random.randn(N, T, 2)  # Two additional random covariates, not necessarily needed by the NN\n",
    "# The NN input will be the 4 transformed versions of x, plus these 2 random covariates\n",
    "# total input dimension = 6\n",
    "X_nn_full = []\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        x = X_main[i, t]\n",
    "        row = [x, np.cos(x), np.sin(x)**2, x**2, W[i,t,0], W[i,t,1]]\n",
    "        X_nn_full.append(row)\n",
    "X_nn_full = np.array(X_nn_full)  # shape (N*T, 6)\n",
    "\n",
    "# Observed covariates Z for random effects:\n",
    "# Let's assume a random intercept model: Z_it = 1 for all i, t\n",
    "Z = np.ones((N, T, p))\n",
    "\n",
    "# True underlying function for the fixed effects (the part the NN tries to capture)\n",
    "# We'll define a \"true\" nonlinear function f_true:\n",
    "def f_true(x):\n",
    "    # match the first four transformations we considered\n",
    "    return x[0] + np.cos(x[0]) + (np.sin(x[0])**2) + 0.5*(x[0]**2)\n",
    "\n",
    "# Generate random effects beta_i ~ N(0, tau^2)\n",
    "beta = np.random.normal(0, tau, size=(N, p))\n",
    "\n",
    "# Generate Y\n",
    "Y = np.zeros((N, T))\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        # X vector for the true function (just the first 4 features)\n",
    "        x_vec = [X_nn_full[i*T + t,0], X_nn_full[i*T + t,1], X_nn_full[i*T + t,2], X_nn_full[i*T + t,3]]\n",
    "        mu = f_true(x_vec) + Z[i,t,:].dot(beta[i])  # random intercept added\n",
    "        Y[i,t] = mu + np.random.normal(0, sigma)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_torch = torch.tensor(X_nn_full, dtype=torch.float32)   # shape (N*T, 6)\n",
    "Y_torch = torch.tensor(Y.reshape(-1,1), dtype=torch.float32)  # shape (N*T, 1)\n",
    "subject_indices = np.repeat(np.arange(N), T)\n",
    "subject_indices_torch = torch.tensor(subject_indices, dtype=torch.long)\n",
    "\n",
    "#############################################\n",
    "# Baseline Model for Comparison (Linear Model)\n",
    "#############################################\n",
    "\n",
    "# Let's compare with a simple linear model with no random effects:\n",
    "# Y ~ a + b1*X + b2*cos(X) + b3*sin^2(X) + b4*X^2 (ignoring W)\n",
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We'll only consider the first 4 features (x, cos(x), sin^2(x), x^2)\n",
    "        self.linear = nn.Linear(4, 1, bias=True)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x[:, :4])\n",
    "\n",
    "linear_model = LinearBaseline()\n",
    "optimizer_linear = optim.Adam(linear_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the baseline linear model just by MSE\n",
    "for epoch in range(200):\n",
    "    optimizer_linear.zero_grad()\n",
    "    pred_lin = linear_model(X_torch)\n",
    "    loss_lin = ((Y_torch - pred_lin)**2).mean()\n",
    "    loss_lin.backward()\n",
    "    optimizer_linear.step()\n",
    "\n",
    "lin_pred = linear_model(X_torch).detach().numpy()\n",
    "lin_mse = np.mean((Y_torch.detach().numpy() - lin_pred)**2)\n",
    "print(\"Baseline Linear Model MSE:\", lin_mse)\n",
    "\n",
    "#############################################\n",
    "# Neural Network Model with Random Effects\n",
    "#############################################\n",
    "\n",
    "# Define the NN for f_NN(X; Theta)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=20):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "nn_model = NeuralNet(input_dim=6, hidden_dim=20)\n",
    "\n",
    "# Random effects gamma_param: We'll estimate them as well\n",
    "# In practice, we'd do EM or Laplace approx. Here, we treat them as parameters to be optimized.\n",
    "gamma_param = nn.Parameter(torch.zeros(N, p))\n",
    "\n",
    "# Parameters for sigma^2 and tau^2 (variance components)\n",
    "# We'll just fix these for simplicity, or we can try to learn them.\n",
    "# Let's assume sigma and tau are known for demonstration. \n",
    "# For a full solution, we would also optimize them.\n",
    "sigma_est = sigma  # fixed\n",
    "tau_est = tau       # fixed\n",
    "Sigma_beta = tau_est**2 * torch.eye(p)\n",
    "\n",
    "# Optimizer for NN and random effects\n",
    "optimizer_nn = optim.Adam(list(nn_model.parameters()) + [gamma_param], lr=0.01)\n",
    "\n",
    "#############################################\n",
    "# Negative Log-Likelihood Function\n",
    "#############################################\n",
    "# Given the model:\n",
    "# Y = f_NN(X;Theta) + Z*beta_i + eps\n",
    "# beta_i ~ N(0, tau^2), eps ~ N(0, sigma^2)\n",
    "#\n",
    "# Conditional on gamma_param (which we use to represent beta_i), the negative log-likelihood:\n",
    "# NLL = 0.5*sum((Y - f_NN(X) - Z*gamma)^2)/sigma^2 + 0.5*N*log(2*pi*sigma^2)\n",
    "#    + 0.5*sum(gamma_i^2)/tau^2 + 0.5*p*N*log(2*pi*tau^2)\n",
    "#\n",
    "# In a true EM, we'd integrate out gamma, but here we approximate by direct optimization.\n",
    "def nll(Y, X, gamma_param, nn_model, sigma, tau):\n",
    "    f = nn_model(X)  # f_NN(X;Theta)\n",
    "    pred = f + gamma_param[subject_indices_torch]  # Z=1 so just add gamma\n",
    "    \n",
    "    # Data log-likelihood\n",
    "    resid = Y - pred\n",
    "    n = Y.shape[0]\n",
    "    ll_data = -0.5*n*torch.log(torch.tensor(2*np.pi*(sigma**2))) - 0.5*(resid**2).sum()/(sigma**2)\n",
    "    \n",
    "    # Prior on gamma (random effects)\n",
    "    # gamma ~ N(0, tau^2)\n",
    "    ll_prior = -0.5*N*p*torch.log(torch.tensor(2*np.pi*(tau**2))) - 0.5*(gamma_param**2).sum()/(tau**2)\n",
    "    \n",
    "    # Negative log-likelihood is - (ll_data + ll_prior)\n",
    "    return -(ll_data + ll_prior)\n",
    "\n",
    "#############################################\n",
    "# Training the NN model with random effects\n",
    "#############################################\n",
    "for epoch in range(1000):\n",
    "    optimizer_nn.zero_grad()\n",
    "    loss = nll(Y_torch, X_torch, gamma_param, nn_model, sigma_est, tau_est)\n",
    "    loss.backward()\n",
    "    optimizer_nn.step()\n",
    "    \n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}, NLL: {loss.item():.4f}\")\n",
    "\n",
    "# After training:\n",
    "nn_pred = (nn_model(X_torch) + gamma_param[subject_indices_torch]).detach().numpy()\n",
    "nn_mse = np.mean((Y_torch.detach().numpy() - nn_pred)**2)\n",
    "print(\"NN + Random Effects MSE:\", nn_mse)\n",
    "\n",
    "#############################################\n",
    "# Results and Comparison\n",
    "#############################################\n",
    "print(\"Comparison of Models:\")\n",
    "print(f\" - Baseline Linear Model MSE: {lin_mse:.4f}\")\n",
    "print(f\" - NN + Random Effects MSE: {nn_mse:.4f}\")\n",
    "\n",
    "# Ideally, NN + random effects should outperform (lower MSE) than the simple linear baseline\n",
    "# if the nonlinear patterns are strong enough and training converged well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec576a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequentist Bivariate Model MSE: 0.11098506\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#############################################\n",
    "# Data Generation\n",
    "#############################################\n",
    "\n",
    "N = 50    # subjects\n",
    "T = 15    # time points per subject\n",
    "p = 1     # dimension of random effects (just random intercept)\n",
    "d_y = 2   # dimension of Y outcome (bivariate)\n",
    "sigma = 0.1  # residual std (for simplicity, assume no covariance, just identity * sigma^2)\n",
    "Sigma_epsilon = sigma**2 * torch.eye(d_y)\n",
    "\n",
    "tau = 0.5  # std dev of random effects for each dimension of random effect\n",
    "\n",
    "# Generate X (single predictor)\n",
    "x_vals = np.linspace(0, 10, T)\n",
    "X_main = np.tile(x_vals, (N, 1))  # shape (N, T)\n",
    "\n",
    "# We'll create a neural network input with a few transformations:\n",
    "# For demonstration, let's do something simple:\n",
    "# Input dimension = 4: [x, cos(x), sin(x)^2, x^2]\n",
    "X_nn_full = []\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        x = X_main[i,t]\n",
    "        row = [x, np.cos(x), np.sin(x)**2, x**2]\n",
    "        X_nn_full.append(row)\n",
    "X_nn_full = np.array(X_nn_full) # (N*T, 4)\n",
    "\n",
    "# Random effects: gamma_i ~ N(0, tau^2)\n",
    "gamma = np.random.normal(0, tau, size=(N, p))\n",
    "\n",
    "# True function for f_NN (simulate something similar to previous)\n",
    "def f_true(x):\n",
    "    return np.array([x[0] + np.cos(x[0]) + np.sin(x[0])**2, \n",
    "                     x[0]**2 * 0.5])  # A 2D output\n",
    "\n",
    "# Generate Y\n",
    "Y = np.zeros((N, T, d_y))\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        x_vec = X_nn_full[i*T + t, :]\n",
    "        # Only the first element of x_vec for the true function's first component, second is just a function of x\n",
    "        mu = f_true([x_vec[0]]) + gamma[i]*1.0  # Z=1\n",
    "        eps = np.random.multivariate_normal(mean=[0,0], cov=(sigma**2*np.eye(d_y)))\n",
    "        Y[i,t,:] = mu + eps\n",
    "\n",
    "# Convert to tensors\n",
    "X_torch = torch.tensor(X_nn_full, dtype=torch.float32)   # (N*T, 4)\n",
    "Y_torch = torch.tensor(Y.reshape(-1, d_y), dtype=torch.float32) # (N*T, 2)\n",
    "subject_indices = torch.tensor(np.repeat(np.arange(N), T), dtype=torch.long)\n",
    "\n",
    "#############################################\n",
    "# Model Definition (NN + random effects)\n",
    "#############################################\n",
    "class BivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=20, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "nn_model = BivariateNN()\n",
    "\n",
    "gamma_param = nn.Parameter(torch.zeros(N, p))\n",
    "\n",
    "optimizer = optim.Adam(list(nn_model.parameters())+[gamma_param], lr=0.01)\n",
    "\n",
    "def nll(Y, X, gamma_param, model, Sigma_epsilon):\n",
    "    # Model prediction\n",
    "    f = model(X) # (N*T, 2)\n",
    "    pred = f + gamma_param[subject_indices] # gamma is (N,1), broadcast to (N*T,1) then added\n",
    "    # pred shape: (N*T,2), gamma only adds to one dimension?\n",
    "    # We defined gamma as scalar random intercept that applies equally to both responses for simplicity.\n",
    "    # If we want it only in first dimension, we can do pred[:,0] += gamma, pred[:,1] unchanged.\n",
    "    # Let's assume random intercept affects both outcomes equally:\n",
    "    # Modify pred to add gamma to both dimensions:\n",
    "    pred = pred + 0.0  # no-op, but we need to be careful with dimensions\n",
    "    # Actually, gamma_param is (N,1). Let's make it (N) and broadcast:\n",
    "    gamma_broad = gamma_param[subject_indices].squeeze(-1)\n",
    "    gamma_2d = gamma_broad.unsqueeze(1).repeat(1,2) # add same intercept to both outcomes\n",
    "    pred = pred + gamma_2d\n",
    "\n",
    "    resid = Y - pred\n",
    "    # NLL under multivariate normal with Sigma_epsilon known:\n",
    "    # logdet(2*pi*Sigma) = 2*log(sigma)*2*pi dimension\n",
    "    # Since Sigma_epsilon = sigma^2 I_2, log|Sigma_epsilon| = 2*log(sigma^2) = 4*log(sigma)\n",
    "    # We'll just compute directly:\n",
    "    dist_term = torch.sum(resid**2)/(sigma**2) # since identity covariance structure\n",
    "    n = Y.shape[0]\n",
    "    d = d_y\n",
    "    ll_data = -0.5 * n * d * np.log(2*np.pi*sigma**2) - 0.5*dist_term\n",
    "\n",
    "    # Prior on gamma: gamma ~ N(0, tau^2)\n",
    "    # log p(gamma)\n",
    "    prior_gamma = -0.5*N*p*np.log(2*np.pi*(tau**2)) - 0.5*torch.sum(gamma_param**2)/(tau**2)\n",
    "\n",
    "    return -(ll_data+prior_gamma)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = nll(Y_torch, X_torch, gamma_param, nn_model, Sigma_epsilon)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "pred = nn_model(X_torch) + gamma_param[subject_indices].repeat(1,2)\n",
    "pred_np = pred.detach().numpy()\n",
    "mse = np.mean((Y_torch.detach().numpy() - pred_np)**2)\n",
    "print(\"Frequentist Bivariate Model MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c3c7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, ELBO: 16978706.740332127\n",
      "Step 200, ELBO: 9303632.133728206\n",
      "Step 300, ELBO: 524039.7157717347\n",
      "Step 400, ELBO: 1063870.1879956722\n",
      "Step 500, ELBO: 591187.8524053693\n",
      "Bayesian Univariate Model MSE: 17.977684020996094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.distributions import constraints\n",
    "\n",
    "pyro.clear_param_store()\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#############################################\n",
    "# Simulate Data (Univariate)\n",
    "#############################################\n",
    "N = 100\n",
    "T = 10\n",
    "p = 1  # random effects dimension\n",
    "sigma = 0.1\n",
    "tau = 0.5\n",
    "\n",
    "x_vals = np.linspace(0, 10, T)\n",
    "X_main = np.tile(x_vals, (N, 1))\n",
    "\n",
    "X = X_main.reshape(N*T, 1)\n",
    "X_torch = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "gamma_true = np.random.normal(0, tau, size=(N,))\n",
    "def f_true(x):\n",
    "    return x[0] + np.cos(x[0]) + (np.sin(x[0])**2)\n",
    "\n",
    "Y = np.zeros(N*T)\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        mu = f_true([X_main[i,t]]) + gamma_true[i]\n",
    "        Y[i*T+t] = mu + np.random.normal(0, sigma)\n",
    "\n",
    "Y_torch = torch.tensor(Y, dtype=torch.float32)\n",
    "subject_indices = torch.tensor(np.repeat(np.arange(N), T), dtype=torch.long)\n",
    "\n",
    "#############################################\n",
    "# Bayesian Model Definition\n",
    "#############################################\n",
    "\n",
    "# We define a simple Bayesian NN with one hidden layer.\n",
    "# We'll place priors on all weights and biases.\n",
    "\n",
    "# Dimensions: input=1, hidden=10, output=1\n",
    "hidden_dim = 10\n",
    "\n",
    "def model(X, Y, subject_idx):\n",
    "    # Plate for subjects: we have N subjects\n",
    "    # gamma ~ Normal(0, tau)\n",
    "    # We'll consider gamma_i as independent across subjects\n",
    "    with pyro.plate(\"subjects\", N, dim=-1):\n",
    "        gamma = pyro.sample(\"gamma\", dist.Normal(torch.tensor(0.0), tau))\n",
    "    # gamma now has shape [N]\n",
    "\n",
    "    # Priors for weights:\n",
    "    fc1_w = pyro.sample(\"fc1_w\", dist.Normal(torch.zeros(hidden_dim, 1), 1.0).to_event(2))\n",
    "    fc1_b = pyro.sample(\"fc1_b\", dist.Normal(torch.zeros(hidden_dim), 1.0).to_event(1))\n",
    "    fc2_w = pyro.sample(\"fc2_w\", dist.Normal(torch.zeros(1, hidden_dim), 1.0).to_event(2))\n",
    "    fc2_b = pyro.sample(\"fc2_b\", dist.Normal(torch.zeros(1), 1.0))\n",
    "\n",
    "    # Compute predictions\n",
    "    # X: (N*T,1)\n",
    "    # fc1_w: (hidden_dim,1), fc1_b: (hidden_dim)\n",
    "    h = torch.relu(X @ fc1_w.transpose(-1,-2) + fc1_b)  # (N*T, hidden_dim)\n",
    "    pred = h @ fc2_w.transpose(-1,-2) + fc2_b  # (N*T,1)\n",
    "    pred = pred.squeeze(-1)  # (N*T,)\n",
    "\n",
    "    # Add gamma:\n",
    "    # subject_idx: (N*T,), gamma: (N,)\n",
    "    # gamma[subject_idx]: (N*T,)\n",
    "    pred = pred + gamma[subject_idx]\n",
    "\n",
    "    # Likelihood\n",
    "    with pyro.plate(\"data\", X.size(0), dim=-1):\n",
    "        pyro.sample(\"obs\", dist.Normal(pred, sigma), obs=Y)\n",
    "\n",
    "def guide(X, Y, subject_idx):\n",
    "    # Define variational parameters\n",
    "    fc1_w_loc = pyro.param(\"fc1_w_loc\", torch.zeros(hidden_dim, 1))\n",
    "    fc1_w_scale = pyro.param(\"fc1_w_scale\", torch.ones(hidden_dim, 1), constraint=constraints.positive)\n",
    "    fc1_b_loc = pyro.param(\"fc1_b_loc\", torch.zeros(hidden_dim))\n",
    "    fc1_b_scale = pyro.param(\"fc1_b_scale\", torch.ones(hidden_dim), constraint=constraints.positive)\n",
    "\n",
    "    fc2_w_loc = pyro.param(\"fc2_w_loc\", torch.zeros(1, hidden_dim))\n",
    "    fc2_w_scale = pyro.param(\"fc2_w_scale\", torch.ones(1, hidden_dim), constraint=constraints.positive)\n",
    "    fc2_b_loc = pyro.param(\"fc2_b_loc\", torch.zeros(1))\n",
    "    fc2_b_scale = pyro.param(\"fc2_b_scale\", torch.ones(1), constraint=constraints.positive)\n",
    "\n",
    "    gamma_loc = pyro.param(\"gamma_loc\", torch.zeros(N))\n",
    "    gamma_scale = pyro.param(\"gamma_scale\", torch.ones(N), constraint=constraints.positive)\n",
    "\n",
    "    # Sample statements (ensure shapes match):\n",
    "    with pyro.plate(\"subjects\", N, dim=-1):\n",
    "        pyro.sample(\"gamma\", dist.Normal(gamma_loc, gamma_scale))\n",
    "\n",
    "    pyro.sample(\"fc1_w\", dist.Normal(fc1_w_loc, fc1_w_scale).to_event(2))\n",
    "    pyro.sample(\"fc1_b\", dist.Normal(fc1_b_loc, fc1_b_scale).to_event(1))\n",
    "    pyro.sample(\"fc2_w\", dist.Normal(fc2_w_loc, fc2_w_scale).to_event(2))\n",
    "    pyro.sample(\"fc2_b\", dist.Normal(fc2_b_loc, fc2_b_scale))\n",
    "\n",
    "optimizer = Adam({\"lr\":0.01})\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "for step in range(500):\n",
    "    loss = svi.step(X_torch, Y_torch, subject_indices)\n",
    "    if (step+1)%100==0:\n",
    "        print(f\"Step {step+1}, ELBO: {loss}\")\n",
    "\n",
    "# Posterior mean predictions:\n",
    "with torch.no_grad():\n",
    "    fc1_w_loc = pyro.param(\"fc1_w_loc\")\n",
    "    fc1_b_loc = pyro.param(\"fc1_b_loc\")\n",
    "    fc2_w_loc = pyro.param(\"fc2_w_loc\")\n",
    "    fc2_b_loc = pyro.param(\"fc2_b_loc\")\n",
    "    gamma_loc = pyro.param(\"gamma_loc\")\n",
    "\n",
    "    h = torch.relu(X_torch @ fc1_w_loc.transpose(-1,-2) + fc1_b_loc)\n",
    "    pred_mean = (h @ fc2_w_loc.transpose(-1,-2) + fc2_b_loc).squeeze(-1)\n",
    "    pred_mean = pred_mean + gamma_loc[subject_indices]\n",
    "    mse = torch.mean((Y_torch - pred_mean)**2).item()\n",
    "    print(\"Bayesian Univariate Model MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1c2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, ELBO: 15119495.51012969\n",
      "Step 200, ELBO: 357725.1309299469\n",
      "Step 300, ELBO: 996476.6801431179\n",
      "Step 400, ELBO: 325760.84372878075\n",
      "Step 500, ELBO: 11704605.582691431\n",
      "Bayesian Bivariate Model MSE: 12.710402488708496\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.distributions import constraints\n",
    "\n",
    "pyro.clear_param_store()\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "#############################################\n",
    "# Simulate Bivariate Data\n",
    "#############################################\n",
    "N = 50\n",
    "T = 10\n",
    "d_y = 2\n",
    "sigma = 0.1\n",
    "tau = 0.5\n",
    "\n",
    "x_vals = np.linspace(0, 5, T)\n",
    "X_main = np.tile(x_vals, (N,1))\n",
    "\n",
    "X = []\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        x = X_main[i,t]\n",
    "        X.append([x, np.cos(x), (np.sin(x))**2, x**2])\n",
    "X = np.array(X)\n",
    "X_torch = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "gamma_true = np.random.normal(0, tau, size=N)\n",
    "\n",
    "def f_true_bivariate(x):\n",
    "    return np.array([x[0] + np.cos(x[0]) + np.sin(x[0])**2,\n",
    "                     0.5*(x[0]**2)])\n",
    "\n",
    "Y = np.zeros((N*T, d_y))\n",
    "for i in range(N):\n",
    "    for t in range(T):\n",
    "        mu = f_true_bivariate(X[i*T+t,:]) + gamma_true[i]\n",
    "        eps = np.random.multivariate_normal(mean=[0,0], cov=(sigma**2*np.eye(2)))\n",
    "        Y[i*T+t,:] = mu + eps\n",
    "\n",
    "Y_torch = torch.tensor(Y, dtype=torch.float32)\n",
    "subject_indices = torch.tensor(np.repeat(np.arange(N), T), dtype=torch.long)\n",
    "\n",
    "#############################################\n",
    "# Bayesian Bivariate Model\n",
    "#############################################\n",
    "hidden_dim = 10\n",
    "output_dim = 2\n",
    "\n",
    "def model_bivariate(X, Y, subject_idx):\n",
    "    # Random intercept per subject\n",
    "    with pyro.plate(\"subjects\", N, dim=-1):\n",
    "        gamma = pyro.sample(\"gamma\", dist.Normal(torch.tensor(0.0), tau))\n",
    "\n",
    "    fc1_w = pyro.sample(\"fc1_w\", dist.Normal(torch.zeros(hidden_dim, X.size(1)), 1.0).to_event(2))\n",
    "    fc1_b = pyro.sample(\"fc1_b\", dist.Normal(torch.zeros(hidden_dim), 1.0).to_event(1))\n",
    "    fc2_w = pyro.sample(\"fc2_w\", dist.Normal(torch.zeros(output_dim, hidden_dim), 1.0).to_event(2))\n",
    "    fc2_b = pyro.sample(\"fc2_b\", dist.Normal(torch.zeros(output_dim), 1.0).to_event(1))\n",
    "\n",
    "    # Compute predictions\n",
    "    h = torch.relu(X @ fc1_w.transpose(-1,-2) + fc1_b)\n",
    "    pred = h @ fc2_w.transpose(-1,-2) + fc2_b  # (N*T, 2)\n",
    "\n",
    "    # Add gamma to both outcomes\n",
    "    gamma_2d = gamma[subject_idx].unsqueeze(-1).expand(-1, d_y)\n",
    "    pred = pred + gamma_2d\n",
    "\n",
    "    # Likelihood\n",
    "    Sigma_epsilon = sigma**2 * torch.eye(d_y)\n",
    "    # The \"data\" plate ensures Pyro knows we're dealing with N*T independent observations\n",
    "    with pyro.plate(\"data\", X.size(0), dim=-1):\n",
    "        pyro.sample(\"obs\",\n",
    "                    dist.MultivariateNormal(pred, covariance_matrix=Sigma_epsilon),\n",
    "                    obs=Y)\n",
    "\n",
    "def guide_bivariate(X, Y, subject_idx):\n",
    "    fc1_w_loc = pyro.param(\"fc1_w_loc\", torch.zeros(hidden_dim, X.size(1)))\n",
    "    fc1_w_scale = pyro.param(\"fc1_w_scale\", torch.ones(hidden_dim, X.size(1)), constraint=constraints.positive)\n",
    "    fc1_b_loc = pyro.param(\"fc1_b_loc\", torch.zeros(hidden_dim))\n",
    "    fc1_b_scale = pyro.param(\"fc1_b_scale\", torch.ones(hidden_dim), constraint=constraints.positive)\n",
    "\n",
    "    fc2_w_loc = pyro.param(\"fc2_w_loc\", torch.zeros(output_dim, hidden_dim))\n",
    "    fc2_w_scale = pyro.param(\"fc2_w_scale\", torch.ones(output_dim, hidden_dim), constraint=constraints.positive)\n",
    "    fc2_b_loc = pyro.param(\"fc2_b_loc\", torch.zeros(output_dim))\n",
    "    fc2_b_scale = pyro.param(\"fc2_b_scale\", torch.ones(output_dim), constraint=constraints.positive)\n",
    "\n",
    "    gamma_loc = pyro.param(\"gamma_loc\", torch.zeros(N))\n",
    "    gamma_scale = pyro.param(\"gamma_scale\", torch.ones(N), constraint=constraints.positive)\n",
    "\n",
    "    with pyro.plate(\"subjects\", N, dim=-1):\n",
    "        pyro.sample(\"gamma\", dist.Normal(gamma_loc, gamma_scale))\n",
    "\n",
    "    pyro.sample(\"fc1_w\", dist.Normal(fc1_w_loc, fc1_w_scale).to_event(2))\n",
    "    pyro.sample(\"fc1_b\", dist.Normal(fc1_b_loc, fc1_b_scale).to_event(1))\n",
    "    pyro.sample(\"fc2_w\", dist.Normal(fc2_w_loc, fc2_w_scale).to_event(2))\n",
    "    pyro.sample(\"fc2_b\", dist.Normal(fc2_b_loc, fc2_b_scale).to_event(1))\n",
    "\n",
    "optimizer = Adam({\"lr\":0.01})\n",
    "svi = SVI(model_bivariate, guide_bivariate, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "for step in range(500):\n",
    "    loss = svi.step(X_torch, Y_torch, subject_indices)\n",
    "    if (step+1) % 100 == 0:\n",
    "        print(f\"Step {step+1}, ELBO: {loss}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    fc1_w_loc = pyro.param(\"fc1_w_loc\")\n",
    "    fc1_b_loc = pyro.param(\"fc1_b_loc\")\n",
    "    fc2_w_loc = pyro.param(\"fc2_w_loc\")\n",
    "    fc2_b_loc = pyro.param(\"fc2_b_loc\")\n",
    "    gamma_loc = pyro.param(\"gamma_loc\")\n",
    "\n",
    "    h = torch.relu(X_torch @ fc1_w_loc.transpose(-1,-2) + fc1_b_loc)\n",
    "    pred_mean = h @ fc2_w_loc.transpose(-1,-2) + fc2_b_loc\n",
    "    pred_mean = pred_mean + gamma_loc[subject_indices].unsqueeze(-1).expand(-1,d_y)\n",
    "    mse_bivariate = torch.mean((Y_torch - pred_mean)**2).item()\n",
    "    print(\"Bayesian Bivariate Model MSE:\", mse_bivariate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
