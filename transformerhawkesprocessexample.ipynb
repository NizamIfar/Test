{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b909796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Types:\n",
      " tensor([[2, 3, 0, 2, 2, 3, 0, 0, 2, 1],\n",
      "        [2, 2, 2, 2, 3, 0, 3, 3, 3, 2]])\n",
      "\n",
      "Timestamps:\n",
      " tensor([[ 0.6160, 10.5315, 19.9364, 26.9495, 26.9632, 29.6921, 35.8813, 44.1364,\n",
      "         54.7192, 83.1685],\n",
      "        [ 7.5266, 33.7648, 34.2313, 55.4660, 57.7925, 58.3210, 80.8975, 88.6014,\n",
      "         90.3982, 95.1555]])\n",
      "\n",
      "Intensity (sample output):\n",
      " tensor([[[0.8378, 0.4360, 1.2342, 0.3594],\n",
      "         [0.5869, 0.6222, 0.9905, 0.3738],\n",
      "         [0.2763, 0.2164, 0.7110, 0.3050],\n",
      "         [0.9372, 0.4644, 0.9110, 0.1692],\n",
      "         [0.9372, 0.4645, 0.9102, 0.1686],\n",
      "         [0.6751, 0.5743, 0.6762, 0.2003],\n",
      "         [0.4894, 0.1575, 0.4467, 0.3028],\n",
      "         [0.3457, 0.1447, 0.6296, 0.2348],\n",
      "         [0.8346, 0.5755, 1.0201, 0.1025],\n",
      "         [0.6928, 0.2060, 0.4869, 0.3033]],\n",
      "\n",
      "        [[0.7931, 0.5812, 1.5989, 0.2888],\n",
      "         [1.0565, 0.4085, 0.8247, 0.1955],\n",
      "         [1.0835, 0.4010, 0.7911, 0.1923],\n",
      "         [0.8851, 0.5674, 1.0196, 0.1317],\n",
      "         [0.7669, 0.6580, 0.3445, 0.2227],\n",
      "         [0.4405, 0.2437, 0.2702, 0.2273],\n",
      "         [0.5021, 0.4437, 0.5466, 0.2642],\n",
      "         [0.4457, 0.4761, 0.5892, 0.1997],\n",
      "         [0.5158, 0.4976, 0.4030, 0.1875],\n",
      "         [0.7344, 0.4700, 0.8650, 0.1523]]], grad_fn=<SoftplusBackward0>)\n",
      "\n",
      "Hidden Representations (H):\n",
      " tensor([[[ 4.9111e-01,  6.2420e-01,  1.5348e+00, -1.6594e+00,  6.8186e-01,\n",
      "           5.5795e-01,  7.3005e-02,  4.4951e-02,  4.0730e-01, -3.9417e-01,\n",
      "           1.5986e+00,  1.0285e+00, -3.6899e-01,  1.3543e+00,  4.9165e+00,\n",
      "           1.4128e+00,  3.2054e+00, -4.1851e-01,  9.3837e-01, -1.1170e+00,\n",
      "           2.3602e+00, -4.3598e-01,  1.0807e+00, -7.5977e-01, -1.8834e-02,\n",
      "           3.9032e-01,  1.0975e+00,  4.3881e-01,  8.8604e-01, -9.6442e-01,\n",
      "           2.3000e+00,  8.8968e-01],\n",
      "         [-7.9219e-01, -1.9362e+00,  8.9633e-01, -1.4399e-01,  6.7411e-01,\n",
      "          -1.6953e-01, -1.9341e+00,  6.0407e-01,  1.4097e+00,  5.2855e-01,\n",
      "          -4.9813e-01,  1.1893e-01, -3.4814e-01,  1.1736e+00, -1.0158e+00,\n",
      "           5.8995e-01,  2.7033e+00,  5.1589e-01, -2.1307e+00, -6.7130e-01,\n",
      "           7.7778e-01, -2.0248e+00,  1.3857e+00,  4.1080e-01,  6.7652e-01,\n",
      "          -2.0382e-01,  3.1165e-01,  4.3248e-01, -2.2996e-01,  4.7797e-01,\n",
      "           3.0923e+00,  2.8249e-01],\n",
      "         [ 9.3464e-01,  1.1150e+00,  2.4630e+00, -2.8176e+00,  2.1186e+00,\n",
      "           1.6340e-01, -2.2848e+00, -1.7022e+00, -1.6163e+00,  1.2065e+00,\n",
      "           7.1562e-01,  3.0988e+00,  7.5804e-01,  3.6696e-01, -7.7189e-01,\n",
      "          -3.6776e-01,  1.3803e+00, -8.2725e-01,  1.8685e+00,  2.9299e-01,\n",
      "           1.8280e+00, -2.2790e-01,  3.0653e+00, -7.5557e-01, -8.2427e-01,\n",
      "           1.1019e+00, -1.6480e-01,  6.1357e-01,  2.3902e+00,  6.6347e-02,\n",
      "           5.8064e-01, -4.9172e-01],\n",
      "         [-5.2291e-01,  1.1281e+00, -4.8301e-01, -1.5142e+00, -6.9816e-01,\n",
      "           1.2445e+00, -7.7874e-01, -1.2650e+00, -1.6588e+00, -1.9338e-01,\n",
      "           8.3008e-01,  2.4069e+00, -6.4129e-01,  2.1806e+00,  4.5927e+00,\n",
      "           1.6565e+00,  2.9353e+00,  5.8035e-02,  7.6986e-01, -8.9835e-01,\n",
      "           2.6396e+00, -4.0629e-01,  1.1143e+00, -8.3380e-01,  1.0119e-01,\n",
      "           6.5268e-01,  1.0627e+00,  4.7373e-01,  9.7772e-01, -9.4391e-01,\n",
      "           2.3646e+00,  1.0798e+00],\n",
      "         [-5.3770e-01,  1.1247e+00, -4.8860e-01, -1.5210e+00, -7.0155e-01,\n",
      "           1.2422e+00, -7.7708e-01, -1.2660e+00, -1.6593e+00, -1.9598e-01,\n",
      "           8.3013e-01,  2.4080e+00, -6.4222e-01,  2.1816e+00,  4.5924e+00,\n",
      "           1.6578e+00,  2.9360e+00,  6.0022e-02,  7.6927e-01, -8.9734e-01,\n",
      "           2.6399e+00, -4.0759e-01,  1.1134e+00, -8.3437e-01,  1.0060e-01,\n",
      "           6.5347e-01,  1.0644e+00,  4.7380e-01,  9.7884e-01, -9.4350e-01,\n",
      "           2.3673e+00,  1.0802e+00],\n",
      "         [-5.1834e-01, -1.8376e+00, -6.7324e-01, -7.3527e-01,  9.4456e-01,\n",
      "           1.4174e-01, -9.3370e-01, -1.1153e+00, -2.0383e-01, -3.7230e-01,\n",
      "          -1.4612e+00,  7.2233e-01, -7.2500e-01,  1.5790e+00, -1.0537e+00,\n",
      "           8.3768e-01,  2.5336e+00,  6.9478e-01, -2.1761e+00, -6.8595e-01,\n",
      "           9.2139e-01, -1.9665e+00,  1.4713e+00,  2.4516e-01,  7.4530e-01,\n",
      "          -1.5588e-01,  2.7391e-01,  4.7460e-01, -3.9909e-01,  6.0217e-01,\n",
      "           3.0248e+00,  3.0190e-01],\n",
      "         [-1.9093e-01, -7.4726e-01,  1.9411e+00, -8.6473e-01,  1.4896e+00,\n",
      "          -7.2502e-01, -4.1217e-02, -1.2415e+00, -2.1035e+00,  5.2928e-02,\n",
      "          -6.5070e-02,  3.2392e+00,  6.1576e-01,  9.5139e-01, -8.0326e-01,\n",
      "          -7.8380e-02,  9.3352e-01, -3.9661e-01,  1.6326e+00, -3.8265e-02,\n",
      "           2.0303e+00,  2.4228e-01,  2.9050e+00, -8.6361e-01, -7.6771e-01,\n",
      "           1.1928e+00,  4.0443e-02,  7.3549e-01,  2.2962e+00,  7.4659e-02,\n",
      "           9.2275e-01, -7.9035e-01],\n",
      "         [ 1.2455e+00,  3.0482e-01,  2.8043e+00, -2.1033e+00,  1.3425e+00,\n",
      "           9.5949e-01, -1.1594e+00, -2.1971e-01, -1.4295e+00, -4.3196e-01,\n",
      "          -6.4948e-01,  2.7936e+00,  2.1306e-01,  8.7441e-01, -9.0675e-01,\n",
      "          -2.0865e-01,  9.5306e-01, -3.5592e-01,  1.8112e+00,  2.0290e-01,\n",
      "           1.7087e+00,  1.5113e-01,  3.1190e+00, -7.4066e-01, -7.5737e-01,\n",
      "           1.1503e+00, -2.9113e-01,  8.7625e-01,  2.2509e+00, -1.2041e-01,\n",
      "           8.5941e-01, -6.3266e-01],\n",
      "         [-7.8489e-01, -9.3300e-01,  9.7176e-01, -2.3860e+00, -1.2763e-01,\n",
      "          -7.0877e-01, -2.0521e+00, -2.3825e-01, -2.5109e-03, -1.0940e+00,\n",
      "          -1.6671e-01,  1.4466e+00, -1.4582e+00,  2.4402e+00,  3.9760e+00,\n",
      "           2.1903e+00,  2.7472e+00,  6.5844e-01,  5.9317e-01, -8.2157e-01,\n",
      "           2.0699e+00, -4.3126e-01,  1.1673e+00, -3.9464e-01,  1.7613e-01,\n",
      "           6.8245e-01,  8.4574e-01,  2.4859e-01,  1.2812e+00, -5.7832e-01,\n",
      "           2.7446e+00,  8.7094e-01],\n",
      "         [ 2.0457e-01,  4.7103e-01, -1.2061e+00, -3.7048e-01,  2.8670e-01,\n",
      "          -6.8267e-01, -6.6743e-01,  1.7789e+00,  3.4761e+00,  4.7015e-01,\n",
      "           2.9178e-01, -9.0233e-01, -2.9238e+00,  2.0340e+00, -1.3955e+00,\n",
      "           2.7009e+00,  2.6234e+00,  1.0731e+00, -1.0757e+00,  9.0739e-01,\n",
      "           1.8498e+00, -2.1600e+00,  1.1500e+00,  2.2351e-01,  6.8823e-01,\n",
      "           1.5093e+00,  2.4983e+00,  1.2614e+00,  3.7079e+00, -2.0362e+00,\n",
      "           1.9829e+00,  1.1520e+00]],\n",
      "\n",
      "        [[ 2.2822e-01,  9.1770e-01, -3.9036e-01, -2.8602e+00, -1.3135e+00,\n",
      "           8.6786e-01, -1.1463e+00,  8.6865e-01,  4.3193e-02, -6.8514e-02,\n",
      "           1.6878e+00,  1.4302e+00, -6.3269e-01,  1.6255e+00,  4.7638e+00,\n",
      "           1.3825e+00,  3.5224e+00, -8.1693e-02,  1.1381e+00, -1.0599e+00,\n",
      "           2.3213e+00, -6.1411e-01,  1.1318e+00, -5.3709e-01,  2.0866e-01,\n",
      "           5.0275e-01,  9.3224e-01,  1.0030e+00,  9.0827e-01, -8.5573e-01,\n",
      "           2.3610e+00,  1.4171e+00],\n",
      "         [-1.1610e+00,  9.8038e-01,  9.4092e-01, -1.5702e+00, -6.5763e-01,\n",
      "          -6.2398e-01, -9.7874e-03, -2.1708e-01, -1.6772e+00, -9.3536e-01,\n",
      "           3.8440e-01,  2.1163e+00, -1.0419e+00,  2.1802e+00,  4.3301e+00,\n",
      "           1.9747e+00,  3.1523e+00,  5.0617e-01,  8.5889e-01, -8.5537e-01,\n",
      "           2.5214e+00, -3.2427e-01,  9.1042e-01, -4.8479e-01,  3.9126e-01,\n",
      "           5.7217e-01,  1.0791e+00,  6.2913e-01,  8.8066e-01, -7.6344e-01,\n",
      "           2.4372e+00,  1.3085e+00],\n",
      "         [-1.4595e+00,  5.9986e-01,  8.1419e-01, -1.2606e+00, -5.1155e-01,\n",
      "          -6.4239e-01,  6.5438e-02, -1.7998e-01, -1.6254e+00, -9.2843e-01,\n",
      "           3.8174e-01,  2.0925e+00, -9.8823e-01,  2.2218e+00,  4.3302e+00,\n",
      "           2.0528e+00,  3.1264e+00,  5.3066e-01,  8.0414e-01, -8.7747e-01,\n",
      "           2.5151e+00, -2.9414e-01,  8.5221e-01, -4.8335e-01,  4.1410e-01,\n",
      "           5.8919e-01,  1.0755e+00,  5.8269e-01,  9.2642e-01, -7.3583e-01,\n",
      "           2.4668e+00,  1.2474e+00],\n",
      "         [ 1.3734e-01, -7.3483e-01,  8.8272e-01, -1.9139e+00, -1.6284e-01,\n",
      "          -6.8412e-01, -1.9371e+00, -1.1305e-01, -2.3229e-02, -1.1073e+00,\n",
      "          -3.0739e-01,  1.1406e+00, -1.4615e+00,  2.2244e+00,  3.9044e+00,\n",
      "           2.1702e+00,  2.9731e+00,  7.4251e-01,  7.9469e-01, -8.2928e-01,\n",
      "           2.0767e+00, -2.1014e-01,  1.0995e+00, -2.5994e-01,  4.4820e-01,\n",
      "           6.5110e-01,  5.9237e-01,  6.5301e-01,  1.1579e+00, -4.6264e-01,\n",
      "           2.2837e+00,  1.0878e+00],\n",
      "         [ 3.5312e-02,  2.0630e-01,  4.4146e-01,  1.0397e+00,  2.6434e+00,\n",
      "          -4.3620e-01, -2.1371e+00, -1.1706e+00,  1.4666e+00, -1.0878e+00,\n",
      "          -2.5926e+00, -9.3898e-01, -1.3915e+00,  1.2093e+00, -1.6295e+00,\n",
      "           1.5606e+00,  2.4043e+00,  1.2209e+00, -2.1416e+00, -2.0207e-01,\n",
      "           8.1558e-01, -1.7056e+00,  1.2602e+00,  4.4832e-01,  9.8344e-01,\n",
      "          -1.6654e-01, -1.6662e-01,  9.3648e-01, -1.4733e-01,  6.1953e-01,\n",
      "           2.4503e+00,  1.2844e-01],\n",
      "         [ 2.0505e-02,  1.2435e+00,  1.8523e+00, -7.7605e-01,  2.1771e+00,\n",
      "           4.1687e-02, -1.7084e+00, -2.0835e+00, -2.8759e-01,  4.4394e-02,\n",
      "          -1.0889e+00,  1.7245e+00, -7.7848e-02,  4.4104e-01, -1.5113e+00,\n",
      "           4.0685e-01,  9.3439e-01, -1.7917e-01,  1.8677e+00,  4.6160e-01,\n",
      "           1.9471e+00,  1.1035e-01,  2.8515e+00, -7.2640e-01, -6.4614e-01,\n",
      "           1.3506e+00, -3.8113e-01,  9.9643e-01,  2.4925e+00,  2.2407e-01,\n",
      "           2.3453e-01, -5.8801e-01],\n",
      "         [ 2.0547e-01, -1.6226e+00, -2.9673e-01,  1.1402e+00,  2.4779e+00,\n",
      "           5.8687e-01, -1.7024e+00,  7.0746e-01,  4.2053e-01,  5.3473e-01,\n",
      "          -1.6847e+00, -1.8290e+00, -1.9536e+00,  8.4576e-01, -1.8562e+00,\n",
      "           1.7805e+00,  2.1224e+00,  1.4536e+00, -2.2790e+00, -3.2530e-01,\n",
      "           8.5393e-01, -1.6080e+00,  1.2130e+00,  5.0318e-01,  1.1035e+00,\n",
      "          -7.4434e-02, -3.3638e-01,  1.1101e+00, -2.2768e-01,  4.9757e-01,\n",
      "           2.4789e+00,  2.7046e-02],\n",
      "         [ 4.9365e-01, -2.8772e-01,  7.9693e-01, -3.1121e-01,  5.0426e-01,\n",
      "           4.7169e-01, -2.5397e+00, -1.5304e-01, -3.2078e-01,  9.7956e-02,\n",
      "          -1.2805e+00, -1.4587e+00, -2.1780e+00,  6.3466e-01, -2.1343e+00,\n",
      "           1.5426e+00,  2.2024e+00,  1.4374e+00, -2.1428e+00,  5.0643e-03,\n",
      "           6.4368e-01, -1.5940e+00,  1.5099e+00,  5.5445e-01,  9.9538e-01,\n",
      "          -1.4732e-01, -1.8831e-01,  9.2457e-01, -2.3025e-01,  6.2739e-01,\n",
      "           2.3129e+00,  2.8542e-01],\n",
      "         [-1.0459e+00, -2.7884e-01,  5.9981e-01,  6.2569e-01,  6.2930e-01,\n",
      "          -1.9355e-02, -2.3906e+00, -6.0709e-01, -2.9411e-01, -1.1531e-01,\n",
      "          -1.1642e+00, -1.5616e+00, -2.1690e+00,  5.5499e-01, -2.0984e+00,\n",
      "           1.7346e+00,  2.2037e+00,  1.4966e+00, -2.1812e+00, -6.4868e-02,\n",
      "           8.0318e-01, -1.5623e+00,  1.3163e+00,  4.3150e-01,  9.5695e-01,\n",
      "          -8.1681e-02,  3.1877e-03,  8.3112e-01, -1.7930e-01,  5.2516e-01,\n",
      "           2.4115e+00,  2.2918e-01],\n",
      "         [ 3.8366e-01,  8.9548e-01, -8.8741e-01, -1.8949e+00, -3.8113e-01,\n",
      "          -4.0323e-01, -1.5543e+00, -5.8759e-01, -1.8025e+00, -6.9666e-01,\n",
      "           1.3451e+00,  3.6149e-01, -2.3797e+00,  1.2238e+00,  3.5641e+00,\n",
      "           2.2388e+00,  2.9961e+00,  9.6456e-01,  9.8716e-01, -5.9523e-01,\n",
      "           2.2371e+00, -2.3808e-01,  1.0892e+00, -5.0692e-01,  5.9962e-01,\n",
      "           7.0930e-01,  7.3115e-01,  7.2199e-01,  1.0753e+00, -5.1470e-01,\n",
      "           2.1630e+00,  1.3287e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Attention Weights:\n",
      " tensor([[[0.0893, 0.0727, 0.1150, 0.1095, 0.1094, 0.0891, 0.1056, 0.1246,\n",
      "          0.0820, 0.1028],\n",
      "         [0.0676, 0.1423, 0.1289, 0.0705, 0.0705, 0.1284, 0.1191, 0.1148,\n",
      "          0.0643, 0.0934],\n",
      "         [0.0803, 0.1608, 0.0709, 0.0978, 0.0979, 0.1814, 0.0388, 0.0449,\n",
      "          0.1044, 0.1229],\n",
      "         [0.0794, 0.0758, 0.1226, 0.1117, 0.1117, 0.0798, 0.1185, 0.1395,\n",
      "          0.0709, 0.0901],\n",
      "         [0.0794, 0.0758, 0.1226, 0.1117, 0.1117, 0.0798, 0.1185, 0.1395,\n",
      "          0.0709, 0.0901],\n",
      "         [0.0714, 0.1260, 0.1376, 0.0735, 0.0734, 0.1092, 0.1337, 0.1250,\n",
      "          0.0658, 0.0844],\n",
      "         [0.0774, 0.1462, 0.1150, 0.0854, 0.0854, 0.1529, 0.0601, 0.0710,\n",
      "          0.1050, 0.1016],\n",
      "         [0.0760, 0.1631, 0.0990, 0.0889, 0.0890, 0.1644, 0.0535, 0.0569,\n",
      "          0.0925, 0.1167],\n",
      "         [0.0783, 0.0833, 0.1261, 0.0871, 0.0870, 0.1091, 0.1392, 0.1378,\n",
      "          0.0693, 0.0828],\n",
      "         [0.0877, 0.1492, 0.0714, 0.1203, 0.1204, 0.1142, 0.0676, 0.0696,\n",
      "          0.1103, 0.0894]],\n",
      "\n",
      "        [[0.1024, 0.1086, 0.1065, 0.0897, 0.1034, 0.0955, 0.0851, 0.0948,\n",
      "          0.0911, 0.1229],\n",
      "         [0.1041, 0.0977, 0.0948, 0.0926, 0.0958, 0.1650, 0.0871, 0.0896,\n",
      "          0.0777, 0.0958],\n",
      "         [0.1004, 0.0949, 0.0916, 0.0911, 0.0990, 0.1711, 0.0889, 0.0904,\n",
      "          0.0793, 0.0933],\n",
      "         [0.0904, 0.1024, 0.0989, 0.0857, 0.1099, 0.1387, 0.0806, 0.1003,\n",
      "          0.0994, 0.0937],\n",
      "         [0.0763, 0.0886, 0.0847, 0.0695, 0.1133, 0.1541, 0.1012, 0.1187,\n",
      "          0.1227, 0.0710],\n",
      "         [0.0766, 0.0640, 0.0627, 0.0785, 0.1101, 0.0628, 0.1126, 0.1658,\n",
      "          0.1658, 0.1011],\n",
      "         [0.0757, 0.0848, 0.0813, 0.0705, 0.1180, 0.1221, 0.1028, 0.1335,\n",
      "          0.1421, 0.0692],\n",
      "         [0.0827, 0.0927, 0.0872, 0.0648, 0.1108, 0.1174, 0.1077, 0.1259,\n",
      "          0.1399, 0.0710],\n",
      "         [0.0809, 0.0867, 0.0830, 0.0683, 0.1093, 0.1415, 0.1047, 0.1242,\n",
      "          0.1319, 0.0696],\n",
      "         [0.1025, 0.1239, 0.1219, 0.1006, 0.0883, 0.1126, 0.0771, 0.0855,\n",
      "          0.0790, 0.1087]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Transformer Hawkes Process Example\n",
    "# \n",
    "# This notebook demonstrates a simplified version of the Transformer Hawkes Process. We:\n",
    "# \n",
    "# - Compute a temporal encoding using cosine and sine functions.\n",
    "# - Embed discrete event types (which are one-hot encoded) into a continuous space.\n",
    "# - Combine these representations.\n",
    "# - Use PyTorch’s multi-head self-attention to model dependencies.\n",
    "# - Pass the output through a feed-forward network.\n",
    "# - Compute a simple conditional intensity (using a linear layer and a softplus activation).\n",
    "# \n",
    "# This basic framework can be extended (for example, by replacing the fixed time-dependent term with a nonparametric smoother or incorporating additional covariates via tree regression/BART) to model the excitation function more flexibly.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# %%\n",
    "# Temporal Encoding using sine and cosine functions.\n",
    "# In this implementation, we follow the standard Transformer positional encoding.\n",
    "class TemporalEncoding(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        M: the dimension of the encoding.\n",
    "        \"\"\"\n",
    "        super(TemporalEncoding, self).__init__()\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: a tensor of shape (L,) representing timestamps.\n",
    "        Returns a tensor of shape (L, M) where each row is the temporal encoding.\n",
    "        \"\"\"\n",
    "        L = t.size(0)\n",
    "        # Create a tensor of positions (we treat t as positions)\n",
    "        # Following the standard positional encoding, we compute:\n",
    "        # PE(pos, 2i)   = cos(pos / (10000^(2i/M)))\n",
    "        # PE(pos, 2i+1) = sin(pos / (10000^(2i/M)))\n",
    "        pe = torch.zeros(L, self.M)\n",
    "        position = t.unsqueeze(1)  # shape (L, 1)\n",
    "        div_term = torch.exp(torch.arange(0, self.M, 2, dtype=torch.float32) * \n",
    "                             (-math.log(10000.0) / self.M))\n",
    "        pe[:, 0::2] = torch.cos(position * div_term)\n",
    "        pe[:, 1::2] = torch.sin(position * div_term)\n",
    "        return pe\n",
    "\n",
    "# %%\n",
    "# Event Type Embedding\n",
    "# One-hot encoding is used to represent each discrete event type as a vector with a 1 in the coordinate corresponding to the event type.\n",
    "# This vector is then mapped to a continuous space via an embedding layer.\n",
    "class EventEmbedding(nn.Module):\n",
    "    def __init__(self, num_event_types, M):\n",
    "        \"\"\"\n",
    "        num_event_types: number of discrete event types.\n",
    "        M: embedding dimension.\n",
    "        \"\"\"\n",
    "        super(EventEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_event_types, M)\n",
    "    \n",
    "    def forward(self, event_types):\n",
    "        \"\"\"\n",
    "        event_types: tensor of shape (L,) containing integer indices for event types.\n",
    "        Returns a tensor of shape (L, M) with the continuous embeddings.\n",
    "        \"\"\"\n",
    "        return self.embedding(event_types)\n",
    "\n",
    "# %%\n",
    "# The simplified Transformer Hawkes Process module.\n",
    "# This module combines the event embedding and temporal encoding, applies multi-head self-attention,\n",
    "# then a feed-forward network, and finally computes an intensity using a linear layer plus softplus.\n",
    "class TransformerHawkesProcess(nn.Module):\n",
    "    def __init__(self, num_event_types, M, num_heads, d_ff):\n",
    "        \"\"\"\n",
    "        num_event_types: number of discrete event types.\n",
    "        M: embedding dimension.\n",
    "        num_heads: number of attention heads.\n",
    "        d_ff: dimension of the feed-forward layer.\n",
    "        \"\"\"\n",
    "        super(TransformerHawkesProcess, self).__init__()\n",
    "        self.M = M\n",
    "        self.num_event_types = num_event_types\n",
    "        self.event_embed = EventEmbedding(num_event_types, M)\n",
    "        self.temporal_encoding = TemporalEncoding(M)\n",
    "        # Multi-head attention (we use batch_first=True for convenience)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=M, num_heads=num_heads, batch_first=True)\n",
    "        # A simple position-wise feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(M, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, M)\n",
    "        )\n",
    "        # Intensity computation: map the hidden representation to num_event_types outputs.\n",
    "        self.intensity_linear = nn.Linear(M, num_event_types)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, event_types, times, attn_mask=None):\n",
    "        \"\"\"\n",
    "        event_types: tensor of shape (B, L) of integer event types.\n",
    "        times: tensor of shape (B, L) of event timestamps.\n",
    "        attn_mask: optional mask for attention (e.g., to prevent attending to future events).\n",
    "        Returns:\n",
    "          intensity: tensor of shape (B, L, num_event_types), the computed intensities.\n",
    "          H: tensor of shape (B, L, M), the hidden representations.\n",
    "          attn_weights: attention weights from the multi-head attention.\n",
    "        \"\"\"\n",
    "        B, L = event_types.shape\n",
    "        \n",
    "        # Get the event type embeddings: shape (B, L, M)\n",
    "        emb_events = self.event_embed(event_types)\n",
    "        \n",
    "        # Get temporal encodings: we process each sequence in the batch separately.\n",
    "        pe = []\n",
    "        for b in range(B):\n",
    "            pe.append(self.temporal_encoding(times[b]))\n",
    "        pe = torch.stack(pe, dim=0)  # shape (B, L, M)\n",
    "        \n",
    "        # Combined representation: sum of event embedding and temporal encoding.\n",
    "        X = emb_events + pe  # shape (B, L, M)\n",
    "        \n",
    "        # Apply multi-head self-attention.\n",
    "        # PyTorch's MultiheadAttention expects queries, keys, values of shape (B, L, M) when batch_first=True.\n",
    "        attn_output, attn_weights = self.multihead_attn(X, X, X, key_padding_mask=attn_mask)\n",
    "        # Add residual connection\n",
    "        X2 = X + attn_output\n",
    "        \n",
    "        # Apply the feed-forward network with a residual connection.\n",
    "        ffn_output = self.ffn(X2)\n",
    "        H = X2 + ffn_output  # Final hidden representation\n",
    "        \n",
    "        # Compute the intensity.\n",
    "        # Here, we apply a linear transformation to each hidden state and then use softplus to ensure positivity.\n",
    "        intensity_raw = self.intensity_linear(H)  # shape (B, L, num_event_types)\n",
    "        intensity = self.softplus(intensity_raw)\n",
    "        \n",
    "        return intensity, H, attn_weights\n",
    "\n",
    "# %%\n",
    "# Example usage in a Jupyter Notebook cell.\n",
    "if __name__ == '__main__':\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Parameters\n",
    "    num_event_types = 4  # e.g., event types: 0, 1, 2, 3\n",
    "    M = 32               # embedding/encoding dimension\n",
    "    num_heads = 4\n",
    "    d_ff = 64            # feed-forward network dimension\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "\n",
    "    # Generate random event sequences.\n",
    "    # event_types: tensor of shape (B, L) with values in {0, 1, 2, 3}.\n",
    "    event_types = torch.randint(0, num_event_types, (batch_size, seq_length))\n",
    "    \n",
    "    # Generate increasing timestamps for each sequence.\n",
    "    times = torch.zeros(batch_size, seq_length)\n",
    "    for b in range(batch_size):\n",
    "        # Generate random timestamps and sort them (simulate increasing event times)\n",
    "        times[b] = torch.sort(torch.rand(seq_length) * 100)[0]\n",
    "\n",
    "    # Initialize the Transformer Hawkes Process model.\n",
    "    model = TransformerHawkesProcess(num_event_types, M, num_heads, d_ff)\n",
    "    \n",
    "    # Forward pass.\n",
    "    intensity, H, attn_weights = model(event_types, times)\n",
    "    \n",
    "    print(\"Event Types:\\n\", event_types)\n",
    "    print(\"\\nTimestamps:\\n\", times)\n",
    "    print(\"\\nIntensity (sample output):\\n\", intensity)\n",
    "    print(\"\\nHidden Representations (H):\\n\", H)\n",
    "    print(\"\\nAttention Weights:\\n\", attn_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1cf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Model 2: Smoother Transformer Hawkes Process\n",
    "# \n",
    "# This variant extends Model 1 by incorporating a nonparametric smoother on the elapsed time between events.  \n",
    "# \n",
    "# For each event (except the first), we compute:\n",
    "# \n",
    "# \\[\n",
    "# \\Delta t_j = t_j - t_{j-1}.\n",
    "# \\]\n",
    "# \n",
    "# Then we pass \\(\\Delta t_j\\) through a small MLP (our nonparametric smoother) to obtain a scalar \\(s_j\\). This scalar is added to the linear intensity before applying softplus:\n",
    "# \n",
    "# \\[\n",
    "# \\lambda_j = \\text{softplus}\\Bigl(\\text{Linear}(h(t_j)) + s_j\\Bigr).\n",
    "# \\]\n",
    "\n",
    "# %%\n",
    "class SmootherTransformerHawkesProcess(nn.Module):\n",
    "    def __init__(self, num_event_types, M, num_heads, d_ff):\n",
    "        super(SmootherTransformerHawkesProcess, self).__init__()\n",
    "        self.M = M\n",
    "        self.event_embed = EventEmbedding(num_event_types, M)\n",
    "        self.temporal_encoding = TemporalEncoding(M)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=M, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(M, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, M)\n",
    "        )\n",
    "        # Linear intensity computation as in Model 1.\n",
    "        self.intensity_linear = nn.Linear(M, num_event_types)\n",
    "        # Nonparametric smoother: a small MLP that takes a scalar (elapsed time) and outputs a scalar.\n",
    "        self.smoother = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, event_types, times, attn_mask=None):\n",
    "        \"\"\"\n",
    "        event_types: tensor of shape (B, L)\n",
    "        times: tensor of shape (B, L)\n",
    "        Returns:\n",
    "          intensity: (B, L, num_event_types)\n",
    "          H: (B, L, M)\n",
    "          attn_weights: from attention.\n",
    "        \"\"\"\n",
    "        B, L = event_types.shape\n",
    "        emb_events = self.event_embed(event_types)  # (B, L, M)\n",
    "        pe = torch.stack([self.temporal_encoding(times[b]) for b in range(B)], dim=0)  # (B, L, M)\n",
    "        X = emb_events + pe  # (B, L, M)\n",
    "        \n",
    "        # Self-attention.\n",
    "        attn_output, attn_weights = self.multihead_attn(X, X, X, key_padding_mask=attn_mask)\n",
    "        X2 = X + attn_output\n",
    "        ffn_output = self.ffn(X2)\n",
    "        H = X2 + ffn_output  # (B, L, M)\n",
    "        \n",
    "        # Compute the linear part of the intensity.\n",
    "        intensity_raw = self.intensity_linear(H)  # (B, L, num_event_types)\n",
    "        \n",
    "        # Compute elapsed time for each event (set the first elapsed time to zero).\n",
    "        # times: (B, L)\n",
    "        elapsed = torch.zeros_like(times)\n",
    "        elapsed[:, 1:] = times[:, 1:] - times[:, :-1]\n",
    "        # Reshape elapsed to (B, L, 1) and pass through the smoother.\n",
    "        elapsed_unsq = elapsed.unsqueeze(-1)\n",
    "        s = self.smoother(elapsed_unsq)  # (B, L, 1)\n",
    "        \n",
    "        # Add the smoother output to the intensity_raw (broadcasting the scalar to each event type).\n",
    "        intensity_enhanced = intensity_raw + s  # (B, L, num_event_types)\n",
    "        intensity = self.softplus(intensity_enhanced)\n",
    "        \n",
    "        return intensity, H, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94166c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Model 3: Set Aggregation Transformer Hawkes Process\n",
    "# \n",
    "# In this variant, we:\n",
    "# - **Concatenate** the event embedding and temporal encoding (instead of summing them) and then project the result.\n",
    "# - Compute a global context vector via mean pooling over the sequence.\n",
    "# - Combine the global context with the local hidden representation before computing the intensity.\n",
    "# \n",
    "# This approach is inspired by set aggregation methods.\n",
    "\n",
    "# %%\n",
    "class SetAggregationTransformerHawkesProcess(nn.Module):\n",
    "    def __init__(self, num_event_types, M, num_heads, d_ff):\n",
    "        super(SetAggregationTransformerHawkesProcess, self).__init__()\n",
    "        self.M = M\n",
    "        self.event_embed = EventEmbedding(num_event_types, M)\n",
    "        self.temporal_encoding = TemporalEncoding(M)\n",
    "        # Instead of summing, we will concatenate embeddings of size M and temporal encoding of size M, then project to M.\n",
    "        self.proj = nn.Linear(2 * M, M)\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=M, num_heads=num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(M, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, M)\n",
    "        )\n",
    "        # Global context: a linear layer to map the mean-pooled vector to M.\n",
    "        self.global_context = nn.Linear(M, M)\n",
    "        \n",
    "        self.intensity_linear = nn.Linear(M, num_event_types)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, event_types, times, attn_mask=None):\n",
    "        \"\"\"\n",
    "        event_types: (B, L)\n",
    "        times: (B, L)\n",
    "        Returns:\n",
    "          intensity: (B, L, num_event_types)\n",
    "          H: (B, L, M)\n",
    "          attn_weights: from attention.\n",
    "        \"\"\"\n",
    "        B, L = event_types.shape\n",
    "        emb_events = self.event_embed(event_types)  # (B, L, M)\n",
    "        pe = torch.stack([self.temporal_encoding(times[b]) for b in range(B)], dim=0)  # (B, L, M)\n",
    "        # Concatenate along the last dimension: shape becomes (B, L, 2*M)\n",
    "        X_cat = torch.cat([emb_events, pe], dim=-1)\n",
    "        # Project back to dimension M.\n",
    "        X = self.proj(X_cat)  # (B, L, M)\n",
    "        \n",
    "        # Self-attention.\n",
    "        attn_output, attn_weights = self.multihead_attn(X, X, X, key_padding_mask=attn_mask)\n",
    "        X2 = X + attn_output\n",
    "        ffn_output = self.ffn(X2)\n",
    "        H = X2 + ffn_output  # (B, L, M)\n",
    "        \n",
    "        # Compute a global context vector by mean pooling over the sequence.\n",
    "        global_vec = H.mean(dim=1)  # (B, M)\n",
    "        global_context = self.global_context(global_vec)  # (B, M)\n",
    "        # Expand the global context to each time step.\n",
    "        global_context_expanded = global_context.unsqueeze(1).expand(-1, L, -1)\n",
    "        \n",
    "        # Combine local and global information.\n",
    "        H_combined = H + global_context_expanded\n",
    "        \n",
    "        # Compute intensity.\n",
    "        intensity_raw = self.intensity_linear(H_combined)  # (B, L, num_event_types)\n",
    "        intensity = self.softplus(intensity_raw)\n",
    "        \n",
    "        return intensity, H, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c0f166",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BasicTransformerHawkesProcess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m event_types, times \u001b[38;5;241m=\u001b[39m generate_sample_data(batch_size, seq_length, num_event_types)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Model 1: Basic Transformer Hawkes Process\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mBasicTransformerHawkesProcess\u001b[49m(num_event_types, M, num_heads, d_ff)\n\u001b[1;32m     31\u001b[0m intensity1, H1, attn_weights1 \u001b[38;5;241m=\u001b[39m model1(event_types, times)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Model 1: Basic Transformer Hawkes ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicTransformerHawkesProcess' is not defined"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Testing the Three Models\n",
    "# \n",
    "# We now generate sample data and run a forward pass through each of the three models.\n",
    "\n",
    "# %%\n",
    "def generate_sample_data(batch_size, seq_length, num_event_types):\n",
    "    # Generate random event types in {0, ..., num_event_types-1}\n",
    "    event_types = torch.randint(0, num_event_types, (batch_size, seq_length))\n",
    "    # Generate increasing timestamps for each sequence.\n",
    "    times = torch.zeros(batch_size, seq_length)\n",
    "    for b in range(batch_size):\n",
    "        times[b] = torch.sort(torch.rand(seq_length) * 100)[0]\n",
    "    return event_types, times\n",
    "\n",
    "# %%\n",
    "# Parameters\n",
    "num_event_types = 4   # For example, event types: 0, 1, 2, 3\n",
    "M = 32                # Embedding/encoding dimension\n",
    "num_heads = 4\n",
    "d_ff = 64             # Feed-forward network dimension\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "# Generate sample data.\n",
    "event_types, times = generate_sample_data(batch_size, seq_length, num_event_types)\n",
    "\n",
    "# %%\n",
    "# Model 1: Basic Transformer Hawkes Process\n",
    "model1 = BasicTransformerHawkesProcess(num_event_types, M, num_heads, d_ff)\n",
    "intensity1, H1, attn_weights1 = model1(event_types, times)\n",
    "print(\"=== Model 1: Basic Transformer Hawkes ===\")\n",
    "print(\"Event Types:\\n\", event_types)\n",
    "print(\"Timestamps:\\n\", times)\n",
    "print(\"Intensity (Model 1):\\n\", intensity1)\n",
    "print(\"Hidden Representations (H1):\\n\", H1)\n",
    "print(\"Attention Weights (Model 1):\\n\", attn_weights1)\n",
    "\n",
    "# %%\n",
    "# Model 2: Smoother Transformer Hawkes Process\n",
    "model2 = SmootherTransformerHawkesProcess(num_event_types, M, num_heads, d_ff)\n",
    "intensity2, H2, attn_weights2 = model2(event_types, times)\n",
    "print(\"\\n=== Model 2: Smoother Transformer Hawkes ===\")\n",
    "print(\"Intensity (Model 2):\\n\", intensity2)\n",
    "print(\"Hidden Representations (H2):\\n\", H2)\n",
    "print(\"Attention Weights (Model 2):\\n\", attn_weights2)\n",
    "\n",
    "# %%\n",
    "# Model 3: Set Aggregation Transformer Hawkes Process\n",
    "model3 = SetAggregationTransformerHawkesProcess(num_event_types, M, num_heads, d_ff)\n",
    "intensity3, H3, attn_weights3 = model3(event_types, times)\n",
    "print(\"\\n=== Model 3: Set Aggregation Transformer Hawkes ===\")\n",
    "print(\"Intensity (Model 3):\\n\", intensity3)\n",
    "print(\"Hidden Representations (H3):\\n\", H3)\n",
    "print(\"Attention Weights (Model 3):\\n\", attn_weights3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
